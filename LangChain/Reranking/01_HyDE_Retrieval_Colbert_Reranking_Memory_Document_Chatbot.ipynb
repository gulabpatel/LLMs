{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYxryrf3LhTx/5NBJqNwuy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/LLMs/blob/main/LangChain/Reranking/01_HyDE_Retrieval_Colbert_Reranking_Memory_Document_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a5bDv0t2dny"
      },
      "outputs": [],
      "source": [
        "%pip install llama-index-core\n",
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-embeddings-openai\n",
        "%pip install llama-index-postprocessor-colbert-rerank\n",
        "%pip install llama-index-readers-web"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
      ],
      "metadata": {
        "id": "1p3v3yFA2fd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Index Construction**\n",
        "\n",
        "As a test, we will index Anthropic's latest documentation about tool/function calling."
      ],
      "metadata": {
        "id": "IkHTnDtTmi8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.readers.web import BeautifulSoupWebReader\n",
        "\n",
        "reader = BeautifulSoupWebReader()\n",
        "\n",
        "documents = reader.load_data(\n",
        "    [\"https://docs.anthropic.com/claude/docs/tool-use\"]\n",
        ")"
      ],
      "metadata": {
        "id": "R7UhAZJf2hkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you inspected the document text, you'd notice that there are way too many blank lines, lets clean that up a bit."
      ],
      "metadata": {
        "id": "k9B8-zBem6rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = documents[0].text.split(\"\\n\")\n",
        "\n",
        "# remove sections with more than two empty lines in a row\n",
        "fixed_lines = [lines[0]]\n",
        "for idx in range(1, len(lines)):\n",
        "    if lines[idx].strip() == \"\" and lines[idx - 1].strip() == \"\":\n",
        "        continue\n",
        "    fixed_lines.append(lines[idx])\n",
        "\n",
        "documents[0].text = \"\\n\".join(fixed_lines)"
      ],
      "metadata": {
        "id": "vrGMV-wcm7T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can create our index using OpenAI embeddings."
      ],
      "metadata": {
        "id": "Qa5SD8W1pNDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    embed_model=OpenAIEmbedding(\n",
        "        model=\"text-embedding-3-large\", embed_batch_size=256\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "oP10SMQ7pNz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Pipeline Contruction**\n",
        "\n",
        "As a demonstration, lets make a robust query pipeline with HyDE for retrieval and Colbert for reranking."
      ],
      "metadata": {
        "id": "znwHhN-Vpa2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_pipeline import (\n",
        "    QueryPipeline,\n",
        "    InputComponent,\n",
        "    ArgPackComponent,\n",
        ")\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
        "\n",
        "# First, we create an input component to capture the user query\n",
        "input_component = InputComponent()\n",
        "\n",
        "# Next, we use the LLM to rewrite a user query\n",
        "rewrite = (\n",
        "    \"Please write a query to a semantic search engine using the current conversation.\\n\"\n",
        "    \"\\n\"\n",
        "    \"\\n\"\n",
        "    \"{chat_history_str}\"\n",
        "    \"\\n\"\n",
        "    \"\\n\"\n",
        "    \"Latest message: {query_str}\\n\"\n",
        "    'Query:\"\"\"\\n'\n",
        ")\n",
        "rewrite_template = PromptTemplate(rewrite)\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature=0.2,\n",
        ")\n",
        "\n",
        "# we will retrieve two times, so we need to pack the retrieved nodes into a single list\n",
        "argpack_component = ArgPackComponent()\n",
        "\n",
        "# using that, we will retrieve...\n",
        "retriever = index.as_retriever(similarity_top_k=6)\n",
        "\n",
        "# then postprocess/rerank with Colbert\n",
        "reranker = ColbertRerank(top_n=3)"
      ],
      "metadata": {
        "id": "K9kBiCxApcn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For generating a response using chat history + retrieved nodes, lets create a custom component."
      ],
      "metadata": {
        "id": "RRhIQQ9Upt2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# then lastly, we need to create a response using the nodes AND chat history\n",
        "from typing import Any, Dict, List, Optional\n",
        "from llama_index.core.bridge.pydantic import Field\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.query_pipeline import CustomQueryComponent\n",
        "from llama_index.core.schema import NodeWithScore\n",
        "\n",
        "DEFAULT_CONTEXT_PROMPT = (\n",
        "    \"Here is some context that may be relevant:\\n\"\n",
        "    \"-----\\n\"\n",
        "    \"{node_context}\\n\"\n",
        "    \"-----\\n\"\n",
        "    \"Please write a response to the following question, using the above context:\\n\"\n",
        "    \"{query_str}\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "class ResponseWithChatHistory(CustomQueryComponent):\n",
        "    llm: OpenAI = Field(..., description=\"OpenAI LLM\")\n",
        "    system_prompt: Optional[str] = Field(\n",
        "        default=None, description=\"System prompt to use for the LLM\"\n",
        "    )\n",
        "    context_prompt: str = Field(\n",
        "        default=DEFAULT_CONTEXT_PROMPT,\n",
        "        description=\"Context prompt to use for the LLM\",\n",
        "    )\n",
        "\n",
        "    def _validate_component_inputs(\n",
        "        self, input: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Validate component inputs during run_component.\"\"\"\n",
        "        # NOTE: this is OPTIONAL but we show you where to do validation as an example\n",
        "        return input\n",
        "\n",
        "    @property\n",
        "    def _input_keys(self) -> set:\n",
        "        \"\"\"Input keys dict.\"\"\"\n",
        "        # NOTE: These are required inputs. If you have optional inputs please override\n",
        "        # `optional_input_keys_dict`\n",
        "        return {\"chat_history\", \"nodes\", \"query_str\"}\n",
        "\n",
        "    @property\n",
        "    def _output_keys(self) -> set:\n",
        "        return {\"response\"}\n",
        "\n",
        "    def _prepare_context(\n",
        "        self,\n",
        "        chat_history: List[ChatMessage],\n",
        "        nodes: List[NodeWithScore],\n",
        "        query_str: str,\n",
        "    ) -> List[ChatMessage]:\n",
        "        node_context = \"\"\n",
        "        for idx, node in enumerate(nodes):\n",
        "            node_text = node.get_content(metadata_mode=\"llm\")\n",
        "            node_context += f\"Context Chunk {idx}:\\n{node_text}\\n\\n\"\n",
        "\n",
        "        formatted_context = self.context_prompt.format(\n",
        "            node_context=node_context, query_str=query_str\n",
        "        )\n",
        "        user_message = ChatMessage(role=\"user\", content=formatted_context)\n",
        "\n",
        "        chat_history.append(user_message)\n",
        "\n",
        "        if self.system_prompt is not None:\n",
        "            chat_history = [\n",
        "                ChatMessage(role=\"system\", content=self.system_prompt)\n",
        "            ] + chat_history\n",
        "\n",
        "        return chat_history\n",
        "\n",
        "    def _run_component(self, **kwargs) -> Dict[str, Any]:\n",
        "        \"\"\"Run the component.\"\"\"\n",
        "        chat_history = kwargs[\"chat_history\"]\n",
        "        nodes = kwargs[\"nodes\"]\n",
        "        query_str = kwargs[\"query_str\"]\n",
        "\n",
        "        prepared_context = self._prepare_context(\n",
        "            chat_history, nodes, query_str\n",
        "        )\n",
        "\n",
        "        response = llm.chat(prepared_context)\n",
        "\n",
        "        return {\"response\": response}\n",
        "\n",
        "    async def _arun_component(self, **kwargs: Any) -> Dict[str, Any]:\n",
        "        \"\"\"Run the component asynchronously.\"\"\"\n",
        "        # NOTE: Optional, but async LLM calls are easy to implement\n",
        "        chat_history = kwargs[\"chat_history\"]\n",
        "        nodes = kwargs[\"nodes\"]\n",
        "        query_str = kwargs[\"query_str\"]\n",
        "\n",
        "        prepared_context = self._prepare_context(\n",
        "            chat_history, nodes, query_str\n",
        "        )\n",
        "\n",
        "        response = await llm.achat(prepared_context)\n",
        "\n",
        "        return {\"response\": response}\n",
        "\n",
        "\n",
        "response_component = ResponseWithChatHistory(\n",
        "    llm=llm,\n",
        "    system_prompt=(\n",
        "        \"You are a Q&A system. You will be provided with the previous chat history, \"\n",
        "        \"as well as possibly relevant context, to assist in answering a user message.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "MuDp-ULBpujJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our modules created, we can link them together in a query pipeline."
      ],
      "metadata": {
        "id": "Xaqx3KBCqCqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = QueryPipeline(\n",
        "    modules={\n",
        "        \"input\": input_component,\n",
        "        \"rewrite_template\": rewrite_template,\n",
        "        \"llm\": llm,\n",
        "        \"rewrite_retriever\": retriever,\n",
        "        \"query_retriever\": retriever,\n",
        "        \"join\": argpack_component,\n",
        "        \"reranker\": reranker,\n",
        "        \"response_component\": response_component,\n",
        "    },\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# run both retrievers -- once with the hallucinated query, once with the real query\n",
        "pipeline.add_link(\n",
        "    \"input\", \"rewrite_template\", src_key=\"query_str\", dest_key=\"query_str\"\n",
        ")\n",
        "pipeline.add_link(\n",
        "    \"input\",\n",
        "    \"rewrite_template\",\n",
        "    src_key=\"chat_history_str\",\n",
        "    dest_key=\"chat_history_str\",\n",
        ")\n",
        "pipeline.add_link(\"rewrite_template\", \"llm\")\n",
        "pipeline.add_link(\"llm\", \"rewrite_retriever\")\n",
        "pipeline.add_link(\"input\", \"query_retriever\", src_key=\"query_str\")\n",
        "\n",
        "# each input to the argpack component needs a dest key -- it can be anything\n",
        "# then, the argpack component will pack all the inputs into a single list\n",
        "pipeline.add_link(\"rewrite_retriever\", \"join\", dest_key=\"rewrite_nodes\")\n",
        "pipeline.add_link(\"query_retriever\", \"join\", dest_key=\"query_nodes\")\n",
        "\n",
        "# reranker needs the packed nodes and the query string\n",
        "pipeline.add_link(\"join\", \"reranker\", dest_key=\"nodes\")\n",
        "pipeline.add_link(\n",
        "    \"input\", \"reranker\", src_key=\"query_str\", dest_key=\"query_str\"\n",
        ")\n",
        "\n",
        "# synthesizer needs the reranked nodes and query str\n",
        "pipeline.add_link(\"reranker\", \"response_component\", dest_key=\"nodes\")\n",
        "pipeline.add_link(\n",
        "    \"input\", \"response_component\", src_key=\"query_str\", dest_key=\"query_str\"\n",
        ")\n",
        "pipeline.add_link(\n",
        "    \"input\",\n",
        "    \"response_component\",\n",
        "    src_key=\"chat_history\",\n",
        "    dest_key=\"chat_history\",\n",
        ")"
      ],
      "metadata": {
        "id": "lJegewJ_qDk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test the pipeline to confirm it works!"
      ],
      "metadata": {
        "id": "sjgm8iS8qmQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running the Pipeline with Memory**\n",
        "\n",
        "\n",
        "The above pipeline uses two inputs -- a query string and a chat_history list.\n",
        "\n",
        "The query string is simply the string input/query.\n",
        "\n",
        "The chat history list is a list of ChatMessage objects. We can use a memory module from llama-index to directly manage and create the memory!"
      ],
      "metadata": {
        "id": "Sml_Q0ydqpgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "pipeline_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
      ],
      "metadata": {
        "id": "-1eFaMxHqm6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets pre-create a \"chat session\" and watch it play out."
      ],
      "metadata": {
        "id": "bBR6ibAXq6mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_inputs = [\n",
        "    \"Hello!\",\n",
        "    \"How does tool-use work with Claude-3 work?\",\n",
        "    \"What models support it?\",\n",
        "    \"Thanks, that what I needed to know!\",\n",
        "]\n",
        "\n",
        "for msg in user_inputs:\n",
        "    # get memory\n",
        "    chat_history = pipeline_memory.get()\n",
        "\n",
        "    # prepare inputs\n",
        "    chat_history_str = \"\\n\".join([str(x) for x in chat_history])\n",
        "\n",
        "    # run pipeline\n",
        "    response = pipeline.run(\n",
        "        query_str=msg,\n",
        "        chat_history=chat_history,\n",
        "        chat_history_str=chat_history_str,\n",
        "    )\n",
        "\n",
        "    # update memory\n",
        "    user_msg = ChatMessage(role=\"user\", content=msg)\n",
        "    pipeline_memory.put(user_msg)\n",
        "    print(str(user_msg))\n",
        "\n",
        "    pipeline_memory.put(response.message)\n",
        "    print(str(response.message))\n",
        "    print()"
      ],
      "metadata": {
        "id": "xSEndVunq7K7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}