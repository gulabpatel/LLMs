{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/LLMs/blob/main/LangChain/LangGraph/02_Corrective_RAG_local_LangGraph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21",
      "metadata": {
        "id": "92ddc4f4-f7bf-4e0e-b5a5-5abd8a008b21"
      },
      "source": [
        "# Corrective RAG (CRAG) -- With Local LLMs\n",
        "\n",
        "## [Code Walkthrough video](https://www.youtube.com/watch?v=E2shqsYwxck&t=109s)\n",
        "Corrective-RAG (CRAG) is a strategy for RAG that incorperates self-reflection / self-grading on retrieved documents.\n",
        "\n",
        "In the paper [here](https://arxiv.org/pdf/2401.15884.pdf), a few steps are taken:\n",
        "\n",
        "* If at least one document exceeds the threshold for relevance, then it proceeds to generation\n",
        "* Before generation, it performns knowledge refinement\n",
        "* This paritions the document into \"knowledge strips\"\n",
        "* It grades each strip, and filters our irrelevant ones\n",
        "* If all documents fall below the relevance threshold or if the grader is unsure, then the framework seeks an additional datasource\n",
        "* It will use web search to supplement retrieval\n",
        "\n",
        "We will implement some of these ideas from scratch using [LangGraph](https://python.langchain.com/docs/langgraph):\n",
        "\n",
        "* Let's skip the knowledge refinement phase as a first pass. This can be added back as a node, if desired.\n",
        "* If *any* documents are irrelevant, let's opt to supplement retrieval with web search.\n",
        "* We'll use [Tavily Search](https://python.langchain.com/docs/integrations/tools/tavily_search) for web search.\n",
        "* Let's use query re-writing to optimize the query for web search.\n",
        "\n",
        "## Running\n",
        "\n",
        "This notebook can be run three ways:\n",
        "\n",
        "(1) Mistral API\n",
        "\n",
        "(2) Locally\n",
        "\n",
        "(3) CoLab: [here](https://colab.research.google.com/drive/1U5OcwWjoXZSud30q4XOk1UlIJNjaD3kX?usp=sharing) is a link to a CoLab for this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba4302f-09d9-4d2a-a18d-a6fd23704850",
      "metadata": {
        "id": "6ba4302f-09d9-4d2a-a18d-a6fd23704850"
      },
      "source": [
        "# Enviorment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a660963-bd3d-4c87-b2e4-b6e432055211",
      "metadata": {
        "id": "4a660963-bd3d-4c87-b2e4-b6e432055211"
      },
      "outputs": [],
      "source": [
        "! pip install --quiet langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python langchain-mistralai gpt4all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728896ab-8aca-4cc3-a152-1491a76bc620",
      "metadata": {
        "id": "728896ab-8aca-4cc3-a152-1491a76bc620"
      },
      "source": [
        "### LLMs\n",
        "\n",
        "You can run this in two ways:\n",
        "\n",
        "(1) Use [Mistral API](https://auth.mistral.ai/ui/login?flow=cc5d3fa5-122b-4c87-bcd8-81e8151c6753).\n",
        "\n",
        "(2) Run locally, as shown below.\n",
        "\n",
        "#### Local Embeddings\n",
        "\n",
        "You can use `GPT4AllEmbeddings()` from Nomic, which can access use Nomic's recently released [v1](https://blog.nomic.ai/posts/nomic-embed-text-v1) and [v1.5](https://blog.nomic.ai/posts/nomic-embed-matryoshka) embeddings.\n",
        "\n",
        "\n",
        "Follow the documentation [here](https://docs.gpt4all.io/gpt4all_python_embedding.html#supported-embedding-models).\n",
        "\n",
        "#### Local LLM\n",
        "\n",
        "(1) Download [Ollama app](https://ollama.ai/).\n",
        "\n",
        "(2) Download a `Mistral` model from various Mistral versions [here](https://ollama.ai/library/mistral) and Mixtral versions [here](https://ollama.ai/library/mixtral) available.\n",
        "```\n",
        "ollama pull mistral\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0cd7ff0-534d-4743-ba6d-e12024a2fd84",
      "metadata": {
        "id": "d0cd7ff0-534d-4743-ba6d-e12024a2fd84"
      },
      "outputs": [],
      "source": [
        "# If using Mistral API\n",
        "mistral_api_key =  <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f74055-d137-466c-9555-4e2da9759ddb",
      "metadata": {
        "id": "83f74055-d137-466c-9555-4e2da9759ddb"
      },
      "source": [
        "### Search\n",
        "\n",
        "We'll use [Tavily Search](https://python.langchain.com/docs/integrations/tools/tavily_search) for web search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28224481-4cb0-4bc6-bf88-2d2b383094df",
      "metadata": {
        "id": "28224481-4cb0-4bc6-bf88-2d2b383094df"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TAVILY_API_KEY'] = <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ec4c0c-65cc-4816-86df-c40b55f9c2d5",
      "metadata": {
        "id": "55ec4c0c-65cc-4816-86df-c40b55f9c2d5"
      },
      "source": [
        "### Tracing\n",
        "\n",
        "Optionally, use [LangSmith](https://docs.smith.langchain.com/) for tracing (shown at bottom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fed362-871a-46df-8ba0-579797ff2e9c",
      "metadata": {
        "id": "68fed362-871a-46df-8ba0-579797ff2e9c"
      },
      "outputs": [],
      "source": [
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = <your-api-key>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c059c3a3-7f01-4d46-8289-fde4c1b4155f",
      "metadata": {
        "id": "c059c3a3-7f01-4d46-8289-fde4c1b4155f"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "Decide to run locally and select LLM to use with Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f4db331-c4d0-4c7c-a9a5-0bebc8a89c6c",
      "metadata": {
        "id": "2f4db331-c4d0-4c7c-a9a5-0bebc8a89c6c"
      },
      "outputs": [],
      "source": [
        "run_local = 'Yes'\n",
        "local_llm = \"mistral:latest\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0",
      "metadata": {
        "id": "6e2b6eed-3b3f-44b5-a34a-4ade1e94caf0"
      },
      "source": [
        "## Index\n",
        "\n",
        "Let's index 3 blog posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8b789b-475b-4e1b-9c66-03504c837830",
      "metadata": {
        "id": "bb8b789b-475b-4e1b-9c66-03504c837830"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "# Load\n",
        "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed and index\n",
        "if run_local == \"Yes\":\n",
        "    embedding = GPT4AllEmbeddings()\n",
        "else:\n",
        "    embedding = MistralAIEmbeddings(mistral_api_key=mistral_api_key)\n",
        "\n",
        "# Index\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=all_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embedding,\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7fd10a-f64a-48de-a116-6d5890def1af",
      "metadata": {
        "id": "fe7fd10a-f64a-48de-a116-6d5890def1af"
      },
      "source": [
        "## LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e75c029-6c10-47c7-871c-1f4932b25309",
      "metadata": {
        "id": "0e75c029-6c10-47c7-871c-1f4932b25309",
        "outputId": "7948f189-2cfb-4766-ef0b-7c06954c4b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'score': 'yes'}\n"
          ]
        }
      ],
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# LLM\n",
        "if run_local == \"Yes\":\n",
        "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
        "else:\n",
        "    llm = ChatMistralAI(\n",
        "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
        "    )\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
        "    Here is the user question: {question} \\n\n",
        "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()\n",
        "question = \"agent memory\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad03302-bd93-43fc-949e-af51a3298cfa",
      "metadata": {
        "id": "dad03302-bd93-43fc-949e-af51a3298cfa",
        "outputId": "31c95960-26be-4765-df39-8734f29c4eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " The given text discusses the concept of building autonomous agents using a large language model (LLM) as its core controller. The text highlights several key components of an LLM-powered agent system, including observation, retrieval, reflection, planning & reacting, and relationships between agents. It also mentions some challenges such as finite context length, long-term planning and task decomposition, and reliability of natural language interface. The text provides examples of proof-of-concept demos like AutoGPT and discusses their limitations. The architecture of the generative agent is also described, which results in emergent social behavior.\n"
          ]
        }
      ],
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "if run_local == \"Yes\":\n",
        "    llm = ChatOllama(model=local_llm, temperature=0)\n",
        "else:\n",
        "    llm = ChatMistralAI(\n",
        "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
        "    )\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b61211-70b5-4471-a714-feb9cc91e860",
      "metadata": {
        "id": "f4b61211-70b5-4471-a714-feb9cc91e860",
        "outputId": "44a5cddf-2111-48d7-eb26-43b89343dde5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' What is agent memory and how can it be effectively utilized in vector database retrieval?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Question Re-writer\n",
        "\n",
        "# LLM\n",
        "if run_local == \"Yes\":\n",
        "    llm = ChatOllama(model=local_llm, temperature=0)\n",
        "else:\n",
        "    llm = ChatMistralAI(\n",
        "        model=\"mistral-medium\", temperature=0, mistral_api_key=mistral_api_key\n",
        "    )\n",
        "\n",
        "# Prompt\n",
        "re_write_prompt = PromptTemplate(\n",
        "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
        "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
        "    input_variables=[\"generation\", \"question\"],\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7fde29-e62e-4445-80f9-122eee0a3922",
      "metadata": {
        "id": "5d7fde29-e62e-4445-80f9-122eee0a3922"
      },
      "source": [
        "## Web Search Tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b36a2f36-bc5f-408d-a5e8-3fa203c233f6",
      "metadata": {
        "id": "b36a2f36-bc5f-408d-a5e8-3fa203c233f6"
      },
      "outputs": [],
      "source": [
        "### Search\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3421cf0-9067-43fe-8681-0d3189d15dd3",
      "metadata": {
        "id": "a3421cf0-9067-43fe-8681-0d3189d15dd3"
      },
      "source": [
        "# Graph\n",
        "\n",
        "Capture the flow in as a graph.\n",
        "\n",
        "## Graph state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e",
      "metadata": {
        "id": "10028794-2fbc-43f9-aa4c-7fe3abd69c1e"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        web_search: whether to add search\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "    question : str\n",
        "    generation : str\n",
        "    web_search : str\n",
        "    documents : List[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d",
      "metadata": {
        "id": "447d1333-082d-479a-a6fa-0ac0df78bb9d"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:ß\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "        grade = score['score']\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    documents.append(web_results)\n",
        "\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "### Edges\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6096626d-dfa5-48e0-8a24-3747b298bc67",
      "metadata": {
        "id": "6096626d-dfa5-48e0-8a24-3747b298bc67"
      },
      "source": [
        "## Build Graph\n",
        "\n",
        "This just follows the flow we outlined in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06",
      "metadata": {
        "id": "0a63776c-f9cd-46ce-b8cf-95c066dc5b06"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
        "workflow.add_edge(\"web_search_node\", \"generate\")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a",
      "metadata": {
        "id": "3ab1d8df-a74e-4b48-a30b-e39bbfd5925a",
        "outputId": "96135ba1-98ba-4f0b-e882-b1db6057b8a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' The given text discusses the concept of building autonomous agents using '\n",
            " 'large language models (LLMs) as their core controllers. LLMs have the '\n",
            " 'potential to be powerful general problem solvers, extending beyond '\n",
            " 'generating well-written copies, stories, essays, and programs. In an '\n",
            " \"LLM-powered agent system, the model functions as the agent's brain, \"\n",
            " 'complemented by several key components: planning, memory, and tool use.\\n'\n",
            " '\\n'\n",
            " '1. Planning: The agent breaks down large tasks into smaller subgoals for '\n",
            " 'efficient handling of complex tasks and can do self-criticism and '\n",
            " 'self-reflection to improve results.\\n'\n",
            " '2. Memory: Short-term memory is utilized for in-context learning, while '\n",
            " 'long-term memory provides the capability to retain and recall information '\n",
            " 'over extended periods by leveraging an external vector store and fast '\n",
            " 'retrieval.\\n'\n",
            " '3. Tool use: The agent learns to call external APIs for missing information, '\n",
            " 'including current information, code execution capability, access to '\n",
            " 'proprietary information sources, and more.\\n'\n",
            " '\\n'\n",
            " 'The text also discusses the types of memory in human brains, including '\n",
            " 'sensory memory, short-term memory (STM), and long-term memory (LTM). Sensory '\n",
            " 'memory provides the ability to retain impressions of sensory information for '\n",
            " 'a few seconds, while STM stores information needed for complex cognitive '\n",
            " 'tasks and lasts for 20-30 seconds. LTM can store information for remarkably '\n",
            " 'long periods with an essentially unlimited storage capacity and has two '\n",
            " 'subtypes: explicit/declarative memory (memory of facts and events) and '\n",
            " 'implicit/procedural memory (skills and routines).\\n'\n",
            " '\\n'\n",
            " 'The text also includes a figure comparing different methods, including AD, '\n",
            " 'ED, source policies, and RL^2, on environments that require memory and '\n",
            " 'exploration.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03ee2be9-2368-46ea-9edd-dc064a7c7c96",
      "metadata": {
        "id": "03ee2be9-2368-46ea-9edd-dc064a7c7c96"
      },
      "source": [
        "Trace:\n",
        "\n",
        "https://smith.langchain.com/public/731df833-57de-4612-8fe8-07cb424bc9a6/r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deb28175-27a1-4afc-9747-2983e87fc881",
      "metadata": {
        "id": "deb28175-27a1-4afc-9747-2983e87fc881",
        "outputId": "02bdff81-4c22-40b6-8ab8-dcff48c8a208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---RETRIEVE---\n",
            "\"Node 'retrieve':\"\n",
            "'\\n---\\n'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "\"Node 'grade_documents':\"\n",
            "'\\n---\\n'\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\n",
            "---TRANSFORM QUERY---\n",
            "\"Node 'transform_query':\"\n",
            "'\\n---\\n'\n",
            "---WEB SEARCH---\n",
            "\"Node 'web_search_node':\"\n",
            "'\\n---\\n'\n",
            "---GENERATE---\n",
            "\"Node 'generate':\"\n",
            "'\\n---\\n'\n",
            "\"Node '__end__':\"\n",
            "'\\n---\\n'\n",
            "(' AlphaCodium is a new approach to code generation by LLMs, proposed in a '\n",
            " 'paper titled \"Code Generation with AlphaCodium: From Prompt Engineering to '\n",
            " 'Flow Engineering.\" It\\'s described as a test-based, multi-stage flow that '\n",
            " 'improves the performance of LLMs on code problems without requiring '\n",
            " 'fine-tuning. The iterative process involves repeatedly running and fixing '\n",
            " 'generated code against input-output tests, with two key elements being '\n",
            " 'generating additional data for the process and enrichment.')\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Run\n",
        "inputs = {\"question\": \"How does the AlphaCodium paper work?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        pprint(f\"Node '{key}':\")\n",
        "        # Optional: print full state at each node\n",
        "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
        "    pprint(\"\\n---\\n\")\n",
        "\n",
        "# Final generation\n",
        "pprint(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598fe73e-f4e6-479f-8d9d-fc81680fff21",
      "metadata": {
        "id": "598fe73e-f4e6-479f-8d9d-fc81680fff21"
      },
      "source": [
        "Trace:\n",
        "\n",
        "https://smith.langchain.com/public/c8b75f1b-38b7-48f2-a399-7ebb969d34f6/r"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
