{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulabpatel/LLMs/blob/main/LangChain/OpenSourceLLM/02_Fastchat_T5_HF_Instructor_Embeddings_Chroma_DB_Multi_Doc_Retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain tiktoken chromadb pypdf transformers InstructorEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbda2c7a-755a-4350-c2af-b9ab3b93599b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.1.16\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Retrieval No Open AI - Fastchat-T5"
      ],
      "metadata": {
        "id": "yAdw3ADcS8Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www.dropbox.com/s/zoj9rnm7oyeaivb/new_papers.zip\n",
        "!unzip -q new_papers.zip -d new_papers"
      ],
      "metadata": {
        "id": "l6XPLPVrqEaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25ce384-f8db-4451-a440-1f6d282a56b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace new_papers/new_papers/toolformer.pdf? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"lmsys/fastchat-t5-3b-v1.0\")"
      ],
      "metadata": {
        "id": "_38HzUkvIewb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=256\n",
        ")\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "6G98f4fXI5kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(local_llm.invoke('What is the capital of England?'))"
      ],
      "metadata": {
        "id": "vZ6FB4R8K42I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "***New Points***\n",
        "- Multiple Files - PDFs\n",
        "- ChromaDB - with more meta data?\n",
        "- Source info\n",
        "- gpt-3.5-turbo API\n",
        "- HuggingFace Embeddings\n",
        "- Instuctor Embeddings\n"
      ],
      "metadata": {
        "id": "7AnZQpL_IZZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up LangChain\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n"
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load multiple and process documents"
      ],
      "metadata": {
        "id": "9UcQKUId3X2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the text files\n",
        "# loader = TextLoader('single_text_file.txt')\n",
        "loader = DirectoryLoader('./new_papers/new_papers/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "PRSeXXc_3Ypj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "vT6KgAIT_BtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "3__nT0D4Fkmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "TlU5AlqY4gwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[3]"
      ],
      "metadata": {
        "id": "Bg6-9jwU4ja_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HF Instructor Embeddings"
      ],
      "metadata": {
        "id": "fhs0C0FYASlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})\n"
      ],
      "metadata": {
        "id": "Emj46ATxtV9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create the DB"
      ],
      "metadata": {
        "id": "YsYsIy8F4cdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "id": "Q_eTIZwf4Dk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None"
      ],
      "metadata": {
        "id": "uRfD_Te-47lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=embedding)"
      ],
      "metadata": {
        "id": "A-h1y_eAHmD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a retriever"
      ],
      "metadata": {
        "id": "siLXR-XT0JoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "6ObunFU30Lxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is Flash attention?\")"
      ],
      "metadata": {
        "id": "cYA-H59u0Skn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "h0iAuh_B0ZjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "Vws1udbPaPmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "jVWgPJXs1yRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_type"
      ],
      "metadata": {
        "id": "H4N0IhRM0hHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_kwargs"
      ],
      "metadata": {
        "id": "4jXL9u-u0prF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a chain"
      ],
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "MGx8XblM4shW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"What is Flash attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "wKfX4vX-5RFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# break it down\n",
        "query = \"What does IO-aware mean?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)\n",
        "# llm_response"
      ],
      "metadata": {
        "id": "olRm73t3rNt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is tiling in flash-attention?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "wg-e6fh6rNwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is toolformer?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "cuFf8D-rrN0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What tools can be used with toolformer?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "yMyu13ouwgqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many examples do we need to provide for each tool?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "t5KETxphrN3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the best retrieval augmentations for LLMs?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "692pHNkFrN5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the differences between REALM and RAG?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "Dl7hy8IvFFL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
      ],
      "metadata": {
        "id": "IPIhZWAR5n3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "id": "3_lp0_796P_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleteing the DB"
      ],
      "metadata": {
        "id": "SSxVCnNi5h1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r db.zip ./db"
      ],
      "metadata": {
        "id": "H7xmepGJ2GAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To cleanup, you can delete the collection\n",
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "# delete the directory\n",
        "!rm -rf db/"
      ],
      "metadata": {
        "id": "Jl84qGQt5Wu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting again loading the db\n",
        "\n",
        "restart the runtime"
      ],
      "metadata": {
        "id": "R2r0ZIBPJp-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip db.zip"
      ],
      "metadata": {
        "id": "8pc7CM_mTQAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "us3F8ZKeRiz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "qK1nY4PkKYGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'db'\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb2 = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=embedding,\n",
        "                   )\n",
        "\n",
        "retriever = vectordb2.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "396RyNbS4EXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "F3vkSxxYKCZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "PsR60NH5KCfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "RWulTG0eKCfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"How much money did Pando raise?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "id": "mDp-g2FtKCfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat prompts"
      ],
      "metadata": {
        "id": "7fPl26c-TbWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "id": "wwyuhrpu5XqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
      ],
      "metadata": {
        "id": "LcWXvSCHRvHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "978QWCeJSRdu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}